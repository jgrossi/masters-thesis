%!TEX root = ../masters.tex

\chapter{Introdução}
\label{cha:introduction}

A necessidade de contribuição acontece de forma natural no ser humano. Os desejos em ajudar ao próximo e contribuir com parte de sua formação é algo que desperta um desejo cada vez mais amplo do ponto de vista social.

Somos seres movidos pela satisfação do outro, e sua conquista, de uma forma ou outra, acarreta em nosso sucesso, nossa satisfação pessoal e de certa forma também profissional. Sentimos atraídos por contribuir e por compartilhar conhecimento, sendo ele umas das principais formas de realização como pessoa.

Com o crescimento da pesquisa e do acesso à informação, um grande número de publicações foi inserido no meio, fazendo com que uma infinidade de material esteja disponível em poucos segundos. Deste modo, a necessidade de centralização automatizada dos dados e a contribuição entre os pesquisadores é inerente ao desenvolvimento desta área. 

No âmbito acadêmico sempre contribuímos de uma forma ou outra com a formação de nossos colegas e parceiros de pesquisa. Esta contribuição pode ser feita através de uma conversa informal ou até mesmo com uma ajuda em documentação ou sugestão de um texto para leitura. Esta sugestão de leitura geralmente possui um caráter muito técnico, e envolve na maioria dos casos a utilização de artigos científicos, objeto central de análise e estudo deste trabalho.

Sabemos da existência de bases de dados de conhecimento nacionais e internacionais, porém, quando estamos falando da contribuição social, em pequena escala, interpessoal, estamos falando de contribuições físicas, com envio de sugestões de artigos para nossos amigos pesquisadores. Este envio é feito de maneira direta, reduzindo o tempo e aumentando consequentemente a praticidade do processo de pesquisa.

Sendo assim, esta experiência como objetivo global permitiria a criação de uma ferramenta de apoio à pesquisa, com pesquisadores compartilhando conhecimentos de maneira informal, anônima e segura. Esta forma de disseminação de conhecimento traria um benefício muito grande do ponto de vista social, uma vez que pesquisadores iriam se unir, mesmo que virtualmente, na transmissão de conhecimento, fazendo do processo de pesquisa um processo mais simplificado, evitando o desperdício de tempo durante a busca por novas descobertas.

Para isso, a utilização de técnicas de extração de metadados deve ser utilizada de maneira eficaz, para que, de forma automática, diversos artigos possam ser analisados e catalogados em pequenos universos de pesquisa. Entende-se por metadados os campos básicos e necessários para que uma pesquisa por título de um artigo, por exemplo, seja feita com sucesso. Resume-se então que os metadados que esperam-se ser extraídos destes artigos são: o título, o nome e e-mail de seus autores, o resumo/\textit{abstract} e as referências utilizadas no texto.

Basicamente estes campos já permitem que uma pesquisa mais detalhada seja feita, e então o artigo localizado com facilidade. Já as referências são necessárias para se criar relações inversas entre autores e suas respectivas citações, facilitando, por exemplo, encontrar artigos semelhantes de uma mesma área do conhecimento.

\begin{figure}
	\centering
	\caption{Processo de Extração de Metadados}
	\label{fig:introduction}
	\includegraphics[width=0.7\linewidth]{./assets/images/introduction}
\end{figure}

\section{Contexto}
\label{sec:context}

A ideia desta pesquisa surgiu de uma necessidade pessoal durante a disciplina de Fundamentos da Ciência da Informação. Era necessária a leitura de um artigo científico de um determinado autor, que não era simples de ser encontrado disponível na Internet, o que fez com que tivesse de recorrer ao professor da disciplina para consegui-lo.

Após o envio deste artigo pelo professor, conforme solicitado, surgiu a seguinte reflexão:

\begin{quote}
	\textit{``Esta contribuição ocorreu de maneira pessoal, entre mim e o professor, porém, se existisse um local onde os pesquisadores pudessem contribuir com o envio destes artigos este processo seria muito mais rápido e eficaz. Seria como se pudéssemos encontrar o artigo instantaneamente, dentro de nossa necessidade temporal. Seria pesquisador ajudando pesquisador.''}
\end{quote}

Desta forma, inclusive por minha formação acadêmica na área de Ciência da Computação, e por minha experiência profissional de mais de uma década em desenvolvimento Web, surgiu a ideia de desenvolver uma ferramenta de contribuição entre pesquisadores, totalmente online e independente. Todavia, estes repositórios de artigos já existiam na Internet e eram bem populados, porém, todos com um certo caráter comercial, com uma empresa ou instituição por trás, objetivando venda e lucro de uma maneira ou outra.

A ideia do desenvolvimento desta ferramenta ecoou durante dias, visto sua ampla aplicabilidade, como também sua contribuição social para com o universo da pesquisa, facilitando a vida de pesquisadores ao redor do mundo, sem complicações. Porém, para sua eficácia no armazenamento dos artigos enviados seria necessária uma análise automatizada dos conteúdos destes arquivos, de maneira a extrair as partes mais importantes, como título, autores e referências, para que pudessem ser catalogados de maneira organizada e estruturada, a fim de facilitar uma busca posterior e permitir uma performance satisfatória por parte do usuário. Estes dados retirados dos documentos analisados são referenciados neste projeto pelo nome de ``metadados'', e são detalhados mais a frente de maneira aprofundada.

Sendo assim iniciou-se minha pesquisa pelo campo da extração de metadados e principalmente pelo \textit{machine learning}, levando ao objeto central deste projeto, de maneira a identificar as características de cada ferramenta disponível atualmente e suas melhores aplicações dentro deste universo. Nada mais justo que contribuir com um projeto de pesquisa tendo como base fundamental o resultado de uma pesquisa acadêmica.

\section{Problema}
\label{sec:problem}

Diversas ferramentas e técnicas para extração de metadados em artigos podem ser facilmente encontradas com uma rápida pesquisa pela Internet. Porém, algumas são propriedade de universidades ou até mesmo de instituições privadas, o que dificulta suas análises e testes, visto que seu código fonte é fechado ou as mesmas não possuem uma forma independente de ser executada.

Este trabalho aborda as principais ferramentas encontradas atualmente, principalmente no que diz respeito à utilização e participação de mercado. Algumas ferramentas não permitem que testes automatizados sejam feitos, visto que não possuem acesso ao código fonte ou não podem ser utilizadas via linha de comando, dificultando portanto a análise dos resultados.

De modo geral, as ferramentas de extração são focadas em \textit{layouts} (disposição visual) pré-definidos, geralmente seguindo modelos de conferências e/ou congressos internacionais, que possuem um padrão visual já estabelecido, como é o caso do IEEE (Institute of Electrical and Electronics Engineers), por exemplo, que serve de referência para diversos outros eventos da área da computação, tomando seu \textit{layout}, a princípio, como base.

Porém, existem diversos outros eventos que possuem \textit{layouts} de artigos considerados fora do padrão e, portanto, necessitam de adaptações por parte destas ferramentas para que seus trabalhos possam ser analisados e catalogados de maneira automática. Esta customização promoveria uma série de benefícios na extração de metadados, abrangendo um número cada vez maior de artigos analisados.

Algumas ferramentas são aparentemente muito eficazes para um certo grupo de artigos, já seguindo um padrão visual pré-determinado. Porém, para alguns \textit{layouts} pouco comuns, de áreas do conhecimento diversas, espera-se que estas ferramentas não sejam tão eficazes, variando de acordo com a tecnologia utilizada e, principalmente, de acordo com o princípio teórico utilizado pelos seus autores, que será mais aprofundado nos próximos capítulos.

Os estudos de comparação já realizados em pesquisa focam os testes em artigos da área de Ciência da Computação, formando uma base viciosa, visto que geralmente os documentos desta área possuem um layout padronizado, facilitando o processo de extração. Assim, um trabalho que teste artigos de diversas áreas, geraria um resultado importante para a pesquisa, podendo realmente analisar a eficácia destas ferramentas empregadas em casos de uso reais, abrangendo diversas áreas do conhecimento e eventos ao redor do mundo.

\section{Justificativa}
\label{sec:justification}

Interessante notar a necessidade de centralização de artigos científicos para o meio acadêmico, além de permitir a contribuição coletiva, que seria uma forma de aumentar cada vez mais o acesso aos materiais de pesquisa. 

Diante disso, esta extração eficaz de metadados de forma automatizada traria benefícios para que estes repositórios de artigos fossem criados e catalogados com precisão, tendo então milhares de documentos em suas bases de dados, prontos para serem pesquisados por pesquisadores de todas as nacionalidades e áreas do conhecimento.

Este trabalho busca, portanto, identificar a melhor maneira que isso pode ser feito, levando em consideração os melhores resultados matemáticos possível, dentro da utilização das técnicas e ferramentas disponíveis atualmente. Este resultado traria, para tanto, um ganho científico considerável, permitindo que novas ferramentas e repositórios fossem criados e a disseminação do conhecimento fosse cada vez maior, e mais rápida.

\section{Histórico}
\label{sec:history}

Os temas ``extração de informação'' e ``machine learning'' são resultantes da fusão de duas áreas do conhecimento bem complementares: a Ciência da Informação e a Ciência da Computação. Por esta proximidade, é muito comum encontrar artigos destes temas em ambas as áreas, porém com focos e objetivos diferentes.

Nesse sentido, essa união se complementa de maneira muito eficaz, visto a característica da fundamentação teórica da Ciência da Informação com a praticidade e desenvolvimento da Ciência da Computação. O resultado é um processo amplo e profundo, que permite que cada vez mais pesquisas sejam feitas, acarretando em um ganho científico cada vez maior.

As pesquisas de \textit{machine learning} se iniciaram muito antes de se tornarem viáveis do ponto de vista computacional e tecnológico. Os primeiros registros destas pesquisas são datados de 1956 por \cite{machine-learning}, onde as primeiras teorias de aprendizado automatizado foram formuladas, utilizando-se para tal de conceitos matemáticos, defendendo a ideia do aprendizado por repetição somado ao reconhecimento de padrões.

Esta fundamentação ainda é muito utilizada e é a base para o desenvolvimento tecnológico existente na área de ``Inteligência Artificial''. Apesar de ser amplamente aplicada, os conceitos por trás desta fundamentação evoluíram ao longo das últimas décadas, juntamente com a capacidade computacional, tornando os resultados muito mais reais e precisos do que era possível imaginar na formulação desta ideia em meados do século.

Diante do cenário atual, com base no acelerado ritmo de desenvolvimento da tecnologia, bem como da evolução das técnicas de \textit{machine learning}, a utilização das ideias apresentadas décadas atrás ainda estão presentes e são amplamente utilizadas para o desenvolvimento de novas tecnologias e ferramentas.

Como é definido por \cite{foundations-machine-learning}, \textit{machine learning} atua como uma forma de aprendizado com base em experiências passadas, através da utilização de dados eletrônicos coletados, que são analisados posteriormente seguindo padrões definidos à máquina, permitindo que ela possa fazer previsões com exatidão com base em sua experiência passada.

Este tema é muito amplo e sua aplicabilidade é extremamente diversificada, permitindo a utilização de suas técnicas e teorias em diversas atividades, como classificação de textos e documentos, processamento de linguagem natural, reconhecimento de fala, detecção de fraudes, diagnósticos médicos e sistemas de recomendações, além de mecanismos de buscas e extração de informação, ponto principal de discussão do presente trabalho.

Dessa forma, este assunto tem sido discutido e pesquisado muito amplamente na Ciência da Informação, tanto nas áreas de classificação de documentos até em aplicações de buscas, unindo os conhecimentos adquiridos desta área com a evolução computacional presente nos dias atuais, onde a Ciência da Computação ganha força e nos permite realizar experimentos mais eficazes.

\section{Objetivos}
\label{sec:goals}

% \subsection{Objetivo Geral}
% \label{ssec:main-goal}

Este trabalho possui como objetivo geral identificar quais são as melhores ferramentas de extração de metadados em artigos científicos e suas melhores aplicações, com base em um conjunto de documentos pré-selecionados para testes, dos mais diversos padrões e de diversas áreas do conhecimento.

Com efeito, esta identificação permite que resultados sejam comparados e confrontados, permitindo decidir qual ferramenta é melhor utilizada para cada padrão visual, abrangendo um conjunto cada vez maior de dados e tendo resultados cada vez mais precisos.

%A necessidade de adequação é uma tendência natural de qualquer ramo de atividade, de maneira a promover possibilidades de ferramentas auto-suficientes capazes de suprir as necessidades de grupos específicos de pesquisas, de eventos ou conferências, que possui padrões de apresentação de artigos personalizados e que demandam de uma análise diferenciada para que possa ser indexada e então analisada por sistemas de informação.

% \subsection{Objetivos Específicos}
% \label{ssec:specific-goals}

Com base na diferenciação dos \textit{layouts} de artigos científicos, este trabalho tem como objetivos específicos identificar também pontos em que as ferramentas de extração de metadados necessitam de adaptações por parte de seus usuários e desenvolvedores, permitindo, desta forma, uma ampliação do universo de aplicação, garantindo assim, uma cobertura mais abrangente dos artigos científicos. Além disso, com base nos resultados coletados, pode-se identificar qual ferramenta é melhor aplicada para cada tipo distinto de metadado.

Acredita-se que os padrões de extração existentes no mercado são, de maneira geral, insuficientes para suprir todos os \emph{layouts} de artigos existentes, limitando a apenas uma pequena parcela destes, dentro de um padrão visual específico, o que acaba gerando uma perda de compatibilidade destas ferramentas existentes atualmente.


\section{Resultados Esperados}
\label{sec:expected-results}

As formas de extração de metadados em artigos científicos são geralmente baseadas em \textit{layouts}, ou seja, em pequenos pedaços de documentos onde certos dados devem estar presentes. Porém, em virtude da grande diversidade de materiais produzidos, dos mais diversos padrões visuais e áreas do conhecimento, este \textit{layout} padrão não se mostra eficiente na abrangência total das necessidades da comunidade científica como um todo. 

Sobre este aspecto, espera-se que certos artigos científicos não tenham seus metadados extraídos de maneira precisa por todas as ferramentas analisadas, uma vez que adaptações no código seriam necessárias a fim de permitir que outros padrões visuais de artigos fossem também reconhecidos, aplicando então um dos fundamentos do \textit{machine learning}, o aprendizado por repetição. 

Por fim, espera-se que, com esta análise aprofundada das ferramentas selecionadas, as mais eficazes na extração de metadados sejam identificadas e analisadas, elevando então o ganho científico neste tema importante para o desenvolvimento tecnológico.


\section{Estrutura}
\label{sec:structure}

Esta pesquisa é estruturada iniciando com uma breve introdução sobre o tema; a apresentação do contexto da pesquisa, bem como sua origem e motivação; a definição do problema; a justificativa; o histórico do trabalho, demonstrando suas origens e fundamentos históricos; os objetivos gerais e específicos e, por fim, os resultados esperados.

O segundo capítulo tem como base o referencial teórico feito através de um mapeamento de bases de dados e eventos científicos relevantes para a área, propondo um caminho de pesquisa eficaz visando atingir os resultados desejáveis com este projeto. Neste capítulo são apresentados alguns conceitos básicos de metadados, além das técnicas mais utilizadas e as ferramentas mais comuns encontradas atualmente.

No terceiro capítulo temos a metodologia para o desenvolvimento deste trabalho, com as ferramentas que serão testadas e principalmente como serão feitos estes testes. Posteriormente, no capítulo quarto temos a análise e apresentação dos resultados, explicando como os testes foram realizados, os ambientes de teste criados e os resultados coletados.

No quinto capítulo temos a discussão/conclusão, trabalhos futuros e considerações finais sobre o trabalho apresentado.


%\section{Limitações do Trabalho}
%
%Este trabalho limita-se aos artigos científicos difundidos na comunidade científica em formato PDF, excluindo aqueles em que seu conteúdo é disponibilizados através de imagens escaneadas de documentos físicos, o que impede, em um primeiro momento, de ter os textos analisados em sua forma original, sem necessidade de processamento extra a fim de obter todo o material textual contido em tais imagens. Além disso o trabalho pressupõe que a língua inglesa seja utilizada como padrão
%
%% Sobre os servidores com Windows
%
%Já na questão de testes de cada técnica de extração de metadados, as técnicas que serão selecionadas deverão ser de código livre/aberto, ou seja, ter seu uso liberado sem a necessidade de pagamento de licenças. Deste modo excluímos todas as técnicas que necessitam de \textit{softwares} proprietários para funcionar, por exigir licenças e fugirem das previsões de teste deste projeto. Assim, os projetos deverão necessariamente utilizar de linguagens de programação livres (ou de código aberto) e que rodem em sistemas operacionais derivados do Unix, como o Linux, por exemplo.

